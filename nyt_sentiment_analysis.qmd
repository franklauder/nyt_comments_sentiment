---
title: "Sentiment Analysis of 2017 New York Times Comments "
format: html
---

# Introduction

# Data Inspection

```{r}

library(dplyr)
library(ggplot2)
library(plotly)

library(tidyr)
library(tidytext)
library(SnowballC)

library(textmineR)
library(textstem)
library(tm)
library(syuzhet)

library(tibble)

library(textclean)
library(furrr)


```


```{r}


nyt_comments17 |> 
    glimpse()


```


```{r}


nyt_comments17<-read.csv("nyt_comments17.csv")


```


```{r}


nyt_comments17 |> head()


```

```{r}

saveRDS(nyt_comments17, "nyt_comments17.rds")


```

```{r}


nyt_comments17_short<-nyt_comments17 |> 
    select(-commentType)



```

We will remove the column commentType


```{r}

nyt_comments17_short |> 
    glimpse()



```

```{r}


saveRDS(nyt_comments17_short, "nyt_comments17_short.rds")


```

# Data Preparation


We will need a function to clean the text in the New York Times commentBody.  




```{r}

clean_comment_text <- function(text) {
  text %>%
    gsub("<.*?>", " ", .) %>%                  # Remove HTML/Markdown tags like <br/>
    gsub("\\\\", " ", .) %>%                   # Remove backslashes
    gsub("[^\\p{L}\\p{N}\\s!?\\p{Emoji_Presentation}]", " ", ., perl = TRUE) %>%  
    gsub("\\s+", " ", .) %>%                   # Replace multiple spaces with single space
    trimws()
}



```

```{r}


nyt_comments_clean<-nyt_comments17_short |> 
    mutate(clean_comments = clean_comment_text(commentBody))


```



```{r}


nyt_comments_clean |> 
    glimpse()



```

```{r}

saveRDS(nyt_comments_clean, "nyt_comments_clean.rds")


```


# Sentiment extraction


```{r}


nyt_comments_clean<-readRDS("nyt_comments_clean.rds")



```

```{r}


nrow(nyt_comments_clean)

```


Because the dataset is relatively large at 969,655 rows, we will use parallel processing for efficency and speed during sentiment extraction. 
 To accomplish this we will use the future_map_dbl function from the the furrr library.  This function helps use cpu cores to parallel tasks. 
 All but one cpu core will be used for our processing. 


  ```{r}
  

plan(multisession, workers = parallel::detectCores() - 1)


  ```

Now that we have set up our parallel processing we will extract sentiments from the clean_comment column of our dataset using the get_sentiment function from the syuzhet library.  The afinn method will be used for labeling the sentiment of each row of comments.  Afinn assigns a sentiment score for each row, ranging fron -5 to 5. 

  ```{r}
  
comments17_sentiment<-nyt_comments_clean |> 
    mutate(
        sentiment_score=future_map_dbl(clean_comments, get_sentiment, method = "afinn")
    )


  ```